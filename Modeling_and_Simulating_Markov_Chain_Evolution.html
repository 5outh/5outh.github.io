<!DOCTYPE html></!DOCTYPE html><html lang="en"><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type" /><meta content="width=device-width, initial-scale=1" name="viewport" /><title>Modeling and Simulating Markov Chain Evolution - Ben Kovach</title><link href="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" rel="icon" /><link href="https://cdn.jsdelivr.net/npm/fomantic-ui@2.8.7/dist/semantic.min.css" rel="stylesheet" /><style type="text/css">body{background-color:#eeeeee !important;font-family:"Ubuntu", serif !important}body .ui.container{font-family:"Ubuntu", serif !important}body h1, h2, h3, h4, h5, h6, .ui.header, .headerFont{font-family:"Kanit", sans-serif !important}body code, pre, tt, .monoFont{font-family:"Roboto Mono","SFMono-Regular","Menlo","Monaco","Consolas","Liberation Mono","Courier New", monospace !important}body div.z-index p.info{color:#808080}body div.z-index ul{list-style-type:square;padding-left:1.5em}body div.z-index .uplinks{margin-left:0.29999em}body div#neuron-theme-default-teal .zettel-content h1{background-color:rgba(0,181,173,0.1)}body div#neuron-theme-default-teal span.zettel-link-container span.zettel-link a{color:#00b5ad}body div#neuron-theme-default-teal span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#00b5ad}body div#neuron-theme-default-teal .deemphasized:hover div.item a:hover{color:#00b5ad !important}body div#neuron-theme-default-teal div#footnotes{border-top-color:#00b5ad}body div#neuron-theme-default-brown .zettel-content h1{background-color:rgba(165,103,63,0.1)}body div#neuron-theme-default-brown span.zettel-link-container span.zettel-link a{color:#a5673f}body div#neuron-theme-default-brown span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#a5673f}body div#neuron-theme-default-brown .deemphasized:hover div.item a:hover{color:#a5673f !important}body div#neuron-theme-default-brown div#footnotes{border-top-color:#a5673f}body div#neuron-theme-default-red .zettel-content h1{background-color:rgba(219,40,40,0.1)}body div#neuron-theme-default-red span.zettel-link-container span.zettel-link a{color:#db2828}body div#neuron-theme-default-red span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#db2828}body div#neuron-theme-default-red .deemphasized:hover div.item a:hover{color:#db2828 !important}body div#neuron-theme-default-red div#footnotes{border-top-color:#db2828}body div#neuron-theme-default-orange .zettel-content h1{background-color:rgba(242,113,28,0.1)}body div#neuron-theme-default-orange span.zettel-link-container span.zettel-link a{color:#f2711c}body div#neuron-theme-default-orange span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#f2711c}body div#neuron-theme-default-orange .deemphasized:hover div.item a:hover{color:#f2711c !important}body div#neuron-theme-default-orange div#footnotes{border-top-color:#f2711c}body div#neuron-theme-default-yellow .zettel-content h1{background-color:rgba(251,189,8,0.1)}body div#neuron-theme-default-yellow span.zettel-link-container span.zettel-link a{color:#fbbd08}body div#neuron-theme-default-yellow span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#fbbd08}body div#neuron-theme-default-yellow .deemphasized:hover div.item a:hover{color:#fbbd08 !important}body div#neuron-theme-default-yellow div#footnotes{border-top-color:#fbbd08}body div#neuron-theme-default-olive .zettel-content h1{background-color:rgba(181,204,24,0.1)}body div#neuron-theme-default-olive span.zettel-link-container span.zettel-link a{color:#b5cc18}body div#neuron-theme-default-olive span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#b5cc18}body div#neuron-theme-default-olive .deemphasized:hover div.item a:hover{color:#b5cc18 !important}body div#neuron-theme-default-olive div#footnotes{border-top-color:#b5cc18}body div#neuron-theme-default-green .zettel-content h1{background-color:rgba(33,186,69,0.1)}body div#neuron-theme-default-green span.zettel-link-container span.zettel-link a{color:#21ba45}body div#neuron-theme-default-green span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#21ba45}body div#neuron-theme-default-green .deemphasized:hover div.item a:hover{color:#21ba45 !important}body div#neuron-theme-default-green div#footnotes{border-top-color:#21ba45}body div#neuron-theme-default-blue .zettel-content h1{background-color:rgba(33,133,208,0.1)}body div#neuron-theme-default-blue span.zettel-link-container span.zettel-link a{color:#2185d0}body div#neuron-theme-default-blue span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#2185d0}body div#neuron-theme-default-blue .deemphasized:hover div.item a:hover{color:#2185d0 !important}body div#neuron-theme-default-blue div#footnotes{border-top-color:#2185d0}body div#neuron-theme-default-violet .zettel-content h1{background-color:rgba(100,53,201,0.1)}body div#neuron-theme-default-violet span.zettel-link-container span.zettel-link a{color:#6435c9}body div#neuron-theme-default-violet span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#6435c9}body div#neuron-theme-default-violet .deemphasized:hover div.item a:hover{color:#6435c9 !important}body div#neuron-theme-default-violet div#footnotes{border-top-color:#6435c9}body div#neuron-theme-default-purple .zettel-content h1{background-color:rgba(163,51,200,0.1)}body div#neuron-theme-default-purple span.zettel-link-container span.zettel-link a{color:#a333c8}body div#neuron-theme-default-purple span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#a333c8}body div#neuron-theme-default-purple .deemphasized:hover div.item a:hover{color:#a333c8 !important}body div#neuron-theme-default-purple div#footnotes{border-top-color:#a333c8}body div#neuron-theme-default-pink .zettel-content h1{background-color:rgba(224,57,151,0.1)}body div#neuron-theme-default-pink span.zettel-link-container span.zettel-link a{color:#e03997}body div#neuron-theme-default-pink span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#e03997}body div#neuron-theme-default-pink .deemphasized:hover div.item a:hover{color:#e03997 !important}body div#neuron-theme-default-pink div#footnotes{border-top-color:#e03997}body div#neuron-theme-default-grey .zettel-content h1{background-color:rgba(118,118,118,0.1)}body div#neuron-theme-default-grey span.zettel-link-container span.zettel-link a{color:#767676}body div#neuron-theme-default-grey span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#767676}body div#neuron-theme-default-grey .deemphasized:hover div.item a:hover{color:#767676 !important}body div#neuron-theme-default-grey div#footnotes{border-top-color:#767676}body div#neuron-theme-default-black .zettel-content h1{background-color:rgba(27,28,29,0.1)}body div#neuron-theme-default-black span.zettel-link-container span.zettel-link a{color:#1b1c1d}body div#neuron-theme-default-black span.zettel-link-container span.zettel-link a:hover{color:#ffffff;background-color:#1b1c1d}body div#neuron-theme-default-black .deemphasized:hover div.item a:hover{color:#1b1c1d !important}body div#neuron-theme-default-black div#footnotes{border-top-color:#1b1c1d}body p{line-height:150%}body img{max-width:100%}body .deemphasized{font-size:0.84999em}body .deemphasized:hover{opacity:1}body .deemphasized:not(:hover){opacity:0.69999}body .deemphasized:not(:hover) a{color:#808080 !important}body div.zettel-view ul{padding-left:1.5em;list-style-type:square}body div.zettel-view .pandoc .highlight{background-color:#ffff00}body div.zettel-view .pandoc .ui.disabled.fitted.checkbox{margin-right:0.29999em;vertical-align:middle}body div.zettel-view .zettel-content .metadata{margin-top:1em}body div.zettel-view .zettel-content .metadata div.date{text-align:center;color:#808080}body div.zettel-view .zettel-content h1{padding-top:0.2em;padding-bottom:0.2em;text-align:center}body div.zettel-view .zettel-content h2{border-bottom:solid 1px #4682b4;margin-bottom:0.5em}body div.zettel-view .zettel-content h3{margin:0px 0px 0.4em 0px}body div.zettel-view .zettel-content h4{opacity:0.8}body div.zettel-view .zettel-content div#footnotes{margin-top:4em;border-top-style:groove;border-top-width:2px;font-size:0.9em}body div.zettel-view .zettel-content aside.footnote-inline{width:30%;padding-left:15px;margin-left:15px;float:right;background-color:#d3d3d3}body div.zettel-view .zettel-content .overflows{overflow:auto}body div.zettel-view .zettel-content code{margin:auto auto auto auto;font-size:100%}body div.zettel-view .zettel-content p code, li code, ol code{padding:0.2em 0.2em 0.2em 0.2em;background-color:#f8f8f8}body div.zettel-view .zettel-content pre{padding:0.5em 0.5em 0.5em 0.5em;overflow:auto;max-width:100%}body div.zettel-view .zettel-content div.pandoc-code{margin-left:auto;margin-right:auto}body div.zettel-view .zettel-content div.pandoc-code pre{background-color:#f8f8f8}body div.zettel-view .zettel-content dl dt{font-weight:bold}body div.zettel-view .zettel-content blockquote{background-color:#f9f9f9;border-left:solid 10px #cccccc;margin:1.5em 0px 1.5em 0px;padding:0.5em 10px 0.5em 10px}body div.zettel-view .zettel-content.raw{background-color:#dddddd}body .tree.flipped{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}body .tree{overflow:auto}body .tree ul.root{padding-top:0px;margin-top:0px}body .tree ul{position:relative;padding:1em 0px 0px 0px;white-space:nowrap;margin:0px auto 0px auto;text-align:center}body .tree ul::after{content:"";display:table;clear:both}body .tree ul:last-child{padding-bottom:0.1em}body .tree li{display:inline-block;vertical-align:top;text-align:center;list-style-type:none;position:relative;padding:1em 0.5em 0em 0.5em}body .tree li::before{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{content:"";position:absolute;top:0px;right:50%;border-top:solid 2px #cccccc;width:50%;height:1.19999em}body .tree li::after{right:auto;left:50%;border-left:solid 2px #cccccc}body .tree li:only-child{padding-top:0em}body .tree li:only-child::after{display:none}body .tree li:only-child::before{display:none}body .tree li:first-child::before{border-style:none;border-width:0px}body .tree li:first-child::after{border-radius:5px 0px 0px 0px}body .tree li:last-child::after{border-style:none;border-width:0px}body .tree li:last-child::before{border-right:solid 2px #cccccc;border-radius:0px 5px 0px 0px}body .tree ul ul::before{content:"";position:absolute;top:0px;left:50%;border-left:solid 2px #cccccc;width:0px;height:1.19999em}body .tree li div.forest-link{border:solid 2px #cccccc;padding:0.2em 0.29999em 0.2em 0.29999em;text-decoration:none;display:inline-block;border-radius:5px 5px 5px 5px;color:#333333;position:relative;top:2px}body .tree.flipped li div.forest-link{-webkit-transform:rotate(180deg);-moz-transform:rotate(180deg);-ms-transform:rotate(180deg);-o-transform:rotate(180deg);transform:rotate(180deg)}body .ui.label.zettel-tag{color:#000000}body .ui.label.zettel-tag a{color:#000000}body nav.backlinks-container{background-color:#eeeeee !important}body ul.backlinks > li{padding-bottom:0.4em;list-style-type:disc}body ul.context-list > li{list-style-type:lower-roman}body span.zettel-link-container span.zettel-link a{font-weight:bold;text-decoration:none}body span.zettel-link-container span.extra{color:auto}body span.zettel-link-container.errors{border:solid 1px #ff0000}body [data-tooltip]:after{font-size:0.69999em}body div.tag-tree div.node{font-weight:bold}body div.tag-tree div.node a.inactive{color:#555555}body .footer-version img{-webkit-filter:grayscale(100%);-moz-filter:grayscale(100%);-ms-filter:grayscale(100%);-o-filter:grayscale(100%);filter:grayscale(100%)}body .footer-version img:hover{-webkit-filter:grayscale(0%);-moz-filter:grayscale(0%);-ms-filter:grayscale(0%);-o-filter:grayscale(0%);filter:grayscale(0%)}body .footer-version, .footer-version a, .footer-version a:visited{color:#808080}body .footer-version a{font-weight:bold}body .footer-version{margin-top:1em !important;font-size:0.69999em}body nav.top-menu{padding-top:1em;padding-bottom:1em;justify-content:center;text-align:center}body nav.top-menu > *{padding-left:0px;padding-right:0px}@media only screen and (max-width: 768px){body div#zettel-container{margin-left:0.4em !important;margin-right:0.4em !important}}</style><link href="https://fonts.googleapis.com/css?family=Kanit|Ubuntu|Roboto+Mono&amp;display=swap" rel="stylesheet" /><script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><meta content="Ben Kovach" name="author" /><meta content="In this post, I will describe and implement a small interface for
modeling Markov chains and simulating their evolution in Haskell.
" name="description" /><link href="https://kovach.me/Modeling_and_Simulating_Markov_Chain_Evolution.html" rel="canonical" /><meta content="Modeling and Simulating Markov Chain Evolution" property="og:title" /><meta content="Ben Kovach" property="og:site_name" /><meta content="article" property="og:type" /><script type="application/ld+json">[]</script><style type="text/css">pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style></head><body><div class="ui fluid container" id="neuron-theme-default-blue"><nav class="top-menu"><div class="ui inverted compact neuron icon menu blue"><a class="left item" href="." title="Home"><i class="home icon"></i></a><a class="left item" href="search.html" title="Search Zettels"><i class="search icon"></i></a><a class="center item" href="https://github.com/5outh/zettelkasten/edit/master/Modeling and Simulating Markov Chain Evolution.md" title="Edit this Zettel"><i class="edit icon"></i></a><a class="right item" href="z-index.html" title="All Zettels (z-index)"><i class="tree icon"></i></a></div></nav><div class="ui text container" id="zettel-container" style="position: relative"><div id="zettel-container-anchor" style="position: absolute; top: -24px; left: 0"></div><div class="zettel-view"><article class="ui raised attached segment zettel-content"><h1>Modeling and Simulating Markov Chain Evolution</h1><div class="pandoc"><p>In this post, I will describe and implement a small interface for modeling Markov chains and simulating their evolution in Haskell.</p><h3 id="what-is-a-markov-chain">What is a Markov Chain?</h3><p>
<small width="200px" style="float:right; text-align:center"><a href="http://commons.wikimedia.org/wiki/File:Markovkate_01.svg#mediaviewer/File:Markovkate_01.svg"><img width="260px" src="http://upload.wikimedia.org/wikipedia/commons/thumb/2/2b/Markovkate_01.svg/1200px-Markovkate_01.svg.png" alt="Markovkate 01.svg"></a><br>A simple two-state Markov chain.<br>image by <a href="//commons.wikimedia.org/wiki/User:Joxemai4" title="User:Joxemai4">Joxemai4</a>.</small>

A [discrete-time Markov chain (DTMC)](http://en.wikipedia.org/wiki/Markov_chain) is a mathematical system that probabalistically transitions between states using only its current state. A Markov chain can be thought of as a directed graph with probabilities for edges and states for vertices. The Markov chain on the right has two states, `E` and `A`. The diagram states that a Markov chain in state `E` will transition back to state `E` with probability `0.3`, and to state `A` with probability `0.7`, and similarly for `A`'s transitions. They can be used for a wide variety of applications in statistical modeling. 
</p>
<p>They can also be used to generate sentences similar to arbitrary blocks of text. We will explore this application towards the end of the post.</p><h3 id="aside-weighted-random-generation">Aside: Weighted Random Generation</h3><p>The first thing that comes to mind when I think of random generation is still <a href="http://learnyouahaskell.com/for-a-few-monads-more#making-monads">the <code>Prob</code> data type described in LYAH</a>. <a href="https://hackage.haskell.org/package/MonadRandom-0.1.13/docs/Control-Monad-Random.html">The <code>MonadRandom</code> library</a> defines a data type <a href="https://hackage.haskell.org/package/MonadRandom-0.1.13/docs/Control-Monad-Random.html#t:Rand"><code>Rand</code></a> which works, in many ways, in the same way as the <code>Prob</code> data type does (with a bit of extension to produce a transormer, etc.). I won’t go into the full details of how this works, but the basic ideas is that, given a list of outcomes with weights, e.g.</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="">[(</span><span class="st">&quot;Heads&quot;</span><span class="">, </span><span class="dv">1</span><span class="">), (</span><span class="st">&quot;Tails&quot;</span><span class="">, </span><span class="dv">1</span><span class="">)]</span>
</code></pre></div><p>we can – by wrapping it in <code>Rand</code> and giving it a random generator – produce a weighted random outcome, “Heads” or “Tails” with the desired weighting. For a concrete example, here’s a functional program written using the <code>MonadRandom</code> library.</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="kw">import</span><span class=""> </span><span class="dt">Control.Monad.Random</span>

<span class="ot">main ::</span><span class=""> </span><span class="dt">IO</span><span class=""> ()</span>
<span class="">main </span><span class="ot">=</span><span class=""> newStdGen </span><span class="op">&gt;&gt;=</span><span class=""> </span><span class="fu">print</span><span class=""> </span><span class="op">.</span><span class=""> evalRand coinFlip</span>
<span class="">  </span><span class="kw">where</span><span class=""> coinFlip </span><span class="ot">=</span><span class=""> uniform [</span><span class="st">&quot;Heads&quot;</span><span class="">, </span><span class="st">&quot;Tails&quot;</span><span class="">]</span>
</code></pre></div><p><code>uniform</code> constructs a <code>Rand</code> from a list with a uniform distribution, i.e. with each member having the same weight. <code>evalRand</code> takes a <code>Rand</code> and a random generator (we’re using <code>StdGen</code> here with <code>newStdGen</code>) and spits out a weighted random object. Running this will print “Heads” about 50% of the time and “Tails” about 50% of the time.</p><p>Also of note is the <code>fromList</code> combinator, which takes a list of objects and their weights and constructs a <code>Rand</code> object. For example, replacing <code>coinFlip</code> with <code>fromList [(&quot;Heads&quot;, 1), (&quot;Tails&quot;, 1)]</code> yields the same program as above.</p><h3 id="markov-chains-an-intermediate-representation">Markov Chains: An Intermediate Representation</h3><p>In order to model Markov chains, we essentially want to build a graph with weighted edges. We can model edge-weighted graphs using a <code>HashMap</code> from vertices to lists of edges, represented as <code>(vertex, weight)</code> pairs (the vertex the edge points to and its weight).</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="kw">import</span><span class=""> </span><span class="kw">qualified</span><span class=""> </span><span class="dt">Data.HashMap.Lazy</span><span class=""> </span><span class="kw">as</span><span class=""> </span><span class="dt">M</span>
<span class="kw">import</span><span class="">           </span><span class="dt">Data.Ratio</span>

<span class="kw">type</span><span class=""> </span><span class="dt">MarkovI</span><span class=""> a </span><span class="ot">=</span><span class=""> </span><span class="dt">M.HashMap</span><span class=""> a (</span><span class="dt">Maybe</span><span class=""> [(a, </span><span class="dt">Rational</span><span class="">)])</span>
</code></pre></div><p>The <code>MarkovI</code> (<code>I</code> for “intermediate”) data type is a synonym for a lazy <code>HashMap</code> from <code>a</code> to a list of vertex-edge pairs. The only difference here is that we allow the list to be empty by using <code>Maybe</code>, which signifies an “end” state in the chain with no outgoing transitions. We could remove this wrapper and use an empty list to signify the same thing, but this representation works better with <code>MonadRandom</code>, since <code>Rand</code>s can’t be empty, making the translation straightforward.</p><p>You might also be wondering why we need an intermediate representation for Markov chains in the first place. The reason for this is that we can’t arbitrarily insert extra objects/weights into <code>Rand</code>s, and we’ll want to build up the mappings piecemeal. We need some intermediate structure to handle this functionality.</p><p>We can define functions to build up <code>MarkovI</code>s via insertion of objects:</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="ot">insertMkvI ::</span><span class=""> (</span><span class="dt">Hashable</span><span class=""> a, </span><span class="dt">Eq</span><span class=""> a) </span><span class="ot">=&gt;</span><span class=""> </span><span class="dt">Rational</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> a </span><span class="ot">-&gt;</span><span class=""> a </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">MarkovI</span><span class=""> a </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">MarkovI</span><span class=""> a</span>
<span class="">insertMkvI r k v mkv </span><span class="ot">=</span><span class=""> M.insert k (</span><span class="dt">Just</span><span class=""> </span><span class="op">$</span><span class=""> </span><span class="kw">case</span><span class=""> M.lookup k mkv </span><span class="kw">of</span>
<span class="">  </span><span class="dt">Nothing</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> [(v, r)]</span>
<span class="">  </span><span class="dt">Just</span><span class=""> xs </span><span class="ot">-&gt;</span><span class=""> </span><span class="kw">case</span><span class=""> xs </span><span class="kw">of</span>
<span class="">    </span><span class="dt">Nothing</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> [(v, r)]</span>
<span class="">    </span><span class="dt">Just</span><span class=""> ys </span><span class="ot">-&gt;</span><span class=""> (v, r)</span><span class="op">:</span><span class="">ys) mkv</span>

<span class="ot">insertEnd ::</span><span class=""> (</span><span class="dt">Hashable</span><span class=""> a, </span><span class="dt">Eq</span><span class=""> a) </span><span class="ot">=&gt;</span><span class=""> a </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">MarkovI</span><span class=""> a </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">MarkovI</span><span class=""> a</span>
<span class="">insertEnd k </span><span class="ot">=</span><span class=""> M.insert k </span><span class="dt">Nothing</span>
</code></pre></div><p><code>insertMkvI</code> inserts an edge into a <code>MarkovI</code>. Its first argument is the weight for the edge being inserted. Its next two arguments are the state objects to add a transition from/to, respectively, and the fourth is the <code>MarkovI</code> to insert into. <code>insertEnd</code> inserts a state with no outbound transitions into a Markov chain.</p><p>It is worth noting that the <code>Rand</code> object constructed from lists like this:</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="">[(</span><span class="dt">True</span><span class="">, </span><span class="dv">1</span><span class="">), (</span><span class="dt">True</span><span class="">, </span><span class="dv">1</span><span class="">), (</span><span class="dt">False</span><span class="">, </span><span class="dv">1</span><span class="">)]</span>
</code></pre></div><p><em>do</em> weight <code>True</code> twice as heavily as <code>False</code>. This will become important later, when talking about a sentence generator.</p><h3 id="markov-chains-final-representation">Markov Chains: Final Representation</h3><p>The final representation of Markov chains simply turns those <code>[(a, Rational)]</code>s in <code>MarkovI</code> into true <code>Rand</code>s.</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="kw">import</span><span class=""> </span><span class="kw">qualified</span><span class=""> </span><span class="dt">Control.Monad.Random</span><span class=""> </span><span class="kw">as</span><span class=""> </span><span class="dt">R</span>

<span class="kw">newtype</span><span class=""> </span><span class="dt">Markov</span><span class=""> g a </span><span class="ot">=</span><span class=""> </span><span class="dt">Markov</span><span class="">{</span><span class="ot"> getMarkov ::</span><span class=""> </span><span class="dt">M.HashMap</span><span class=""> a (</span><span class="dt">Maybe</span><span class=""> (</span><span class="dt">R.Rand</span><span class=""> g a)) }</span>
</code></pre></div><p>We can define a simple conversion function to construct <code>Markov</code>s from the intermediate representation by converting their distribution lists to <code>Rand</code>s via <code>fromList</code>. This is straightforward because empty lists are represented as <code>Nothing</code>, so we don’t have to explicitly deal with that edge case when calling <code>R.fromList</code>, which would normally fail in such a case.</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="ot">fromMarkovI ::</span><span class=""> </span><span class="dt">RandomGen</span><span class=""> g </span><span class="ot">=&gt;</span><span class=""> </span><span class="dt">MarkovI</span><span class=""> a </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">Markov</span><span class=""> g a</span>
<span class="">fromMarkovI </span><span class="ot">=</span><span class=""> </span><span class="dt">Markov</span><span class=""> </span><span class="op">.</span><span class=""> M.map (R.fromList </span><span class="op">&lt;$&gt;</span><span class="">)</span>
</code></pre></div><p>The first goal is to be able to – given a state and a random generator – transition to a new state probabalistically. The second goal is to be able to repeat this <code>n</code> times and track the states we pass through.</p><p><code>runMarkov1</code> accomplishes the first goal:</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="kw">type</span><span class=""> </span><span class="dt">Err</span><span class=""> </span><span class="ot">=</span><span class=""> </span><span class="dt">String</span>
<span class="kw">data</span><span class=""> </span><span class="dt">Outcome</span><span class=""> g a </span><span class="ot">=</span>
<span class="">    </span><span class="dt">Error</span><span class=""> </span><span class="dt">Err</span>
<span class="">  </span><span class="op">|</span><span class=""> </span><span class="dt">Val</span><span class=""> a g</span>
<span class="">  </span><span class="op">|</span><span class=""> </span><span class="dt">End</span>
<span class="">    </span><span class="kw">deriving</span><span class=""> (</span><span class="dt">Show</span><span class="">, </span><span class="dt">Eq</span><span class="">)</span>

<span class="ot">runMarkov1 ::</span><span class=""> (</span><span class="dt">R.RandomGen</span><span class=""> g, </span><span class="dt">Hashable</span><span class=""> a, </span><span class="dt">Eq</span><span class=""> a) </span><span class="ot">=&gt;</span><span class=""> </span><span class="dt">Markov</span><span class=""> g a </span><span class="ot">-&gt;</span><span class=""> g </span><span class="ot">-&gt;</span><span class=""> a </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">Outcome</span><span class=""> g a</span>
<span class="">runMarkov1 mkv gen x </span><span class="ot">=</span><span class=""> </span><span class="kw">case</span><span class=""> M.lookup x (getMarkov mkv) </span><span class="kw">of</span>
<span class="">  </span><span class="dt">Nothing</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">Error</span><span class=""> </span><span class="st">&quot;Internal error; cannot find value&quot;</span>
<span class="">  </span><span class="dt">Just</span><span class=""> rs </span><span class="ot">-&gt;</span><span class=""> </span><span class="kw">case</span><span class=""> </span><span class="fu">flip</span><span class=""> R.runRand gen </span><span class="op">&lt;$&gt;</span><span class=""> rs </span><span class="kw">of</span>
<span class="">    </span><span class="dt">Nothing</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">End</span>
<span class="">    </span><span class="dt">Just</span><span class=""> (a, g) </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">Val</span><span class=""> a g</span>
</code></pre></div><p>First, if the state we’re looking for doesn’t exist, it is impossible to transition out of it, so the computation fails with an internal error. If not, we get a probablistic value out of the transition mappings from the state in question. If there aren’t any, we just <code>End</code> – we cannot transition, but don’t really want to throw an error. If there are values to choose from, we return one along with a new random generator, wrapped in <code>Val</code>.</p><p>Extending this to run <code>n</code> times isn’t too tough. It mostly consists of finagling data types into the representation we want.</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="ot">runMarkov ::</span><span class=""> (</span><span class="dt">R.RandomGen</span><span class=""> g, </span><span class="dt">Hashable</span><span class=""> a, </span><span class="dt">Eq</span><span class=""> a) </span><span class="ot">=&gt;</span><span class=""> </span><span class="dt">Integer</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">Markov</span><span class=""> g a </span><span class="ot">-&gt;</span><span class=""> g </span><span class="ot">-&gt;</span><span class=""> a </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">Either</span><span class=""> </span><span class="dt">Err</span><span class=""> [a]</span>
<span class="">runMarkov n mkv gen x </span><span class="ot">=</span><span class=""> go n</span>
<span class="">  </span><span class="kw">where</span>
<span class="">    go m </span><span class="op">|</span><span class=""> m </span><span class="op">&lt;=</span><span class=""> </span><span class="dv">0</span><span class=""> </span><span class="ot">=</span><span class=""> </span><span class="dt">Right</span><span class=""> []</span>
<span class="">         </span><span class="op">|</span><span class=""> </span><span class="fu">otherwise</span><span class=""> </span><span class="ot">=</span><span class=""> (x</span><span class="op">:</span><span class="">) </span><span class="op">&lt;$&gt;</span><span class=""> </span><span class="kw">case</span><span class=""> runMarkov1 mkv gen x </span><span class="kw">of</span>
<span class="">            </span><span class="dt">Val</span><span class=""> a g </span><span class="ot">-&gt;</span><span class=""> runMarkov (n</span><span class="op">-</span><span class="dv">1</span><span class="">) mkv g a</span>
<span class="">            </span><span class="dt">End</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">Right</span><span class=""> []</span>
<span class="">            </span><span class="dt">Error</span><span class=""> err </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">Left</span><span class=""> err</span>
</code></pre></div><p>If we hit an <code>End</code>, the simulation terminates because it can’t progress any further. If we get an error along the way, we wrap it in <code>Left</code> and return it. Otherwise, we run <code>runMarkov1</code> repeatedly <code>n</code> times, starting from the previously computed state each time, and collecting the results into a list. If no errors occur, the result will be a list of states passed through while the simulation runs.</p><p>We can now define a <code>fromList</code> function, which builds up a Markov chain from mappings represented in list form.</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="ot">fromList ::</span><span class=""> (</span><span class="dt">Hashable</span><span class=""> a, </span><span class="dt">Eq</span><span class=""> a, </span><span class="dt">R.RandomGen</span><span class=""> g) </span><span class="ot">=&gt;</span><span class=""> [(a, [(a, </span><span class="dt">Rational</span><span class="">)])] </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">Markov</span><span class=""> g a</span>
<span class="">fromList </span><span class="ot">=</span><span class=""> </span><span class="dt">Markov</span><span class=""> </span><span class="op">.</span><span class=""> foldl&#39; (</span><span class="fu">flip</span><span class=""> </span><span class="op">$</span><span class=""> </span><span class="fu">uncurry</span><span class=""> ins) M.empty</span>
<span class="">  </span><span class="kw">where</span><span class=""> ins a b m </span><span class="ot">=</span><span class=""> </span><span class="kw">case</span><span class=""> b </span><span class="kw">of</span>
<span class="">          [] </span><span class="ot">-&gt;</span><span class=""> M.insert a </span><span class="dt">Nothing</span><span class=""> m</span>
<span class="">          _  </span><span class="ot">-&gt;</span><span class=""> M.insert a (</span><span class="dt">Just</span><span class=""> </span><span class="op">$</span><span class=""> R.fromList b) m</span>
</code></pre></div><p>With this at our disposal, it’s easy to model and run the example Markov chain I mentioned earlier.</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="ot">example ::</span><span class=""> </span><span class="dt">Markov</span><span class=""> </span><span class="dt">PureMT</span><span class=""> </span><span class="dt">String</span>
<span class="">example </span><span class="ot">=</span><span class=""> fromList [(</span><span class="st">&quot;E&quot;</span><span class="">, [(</span><span class="st">&quot;E&quot;</span><span class="">, </span><span class="dv">3</span><span class="">), (</span><span class="st">&quot;A&quot;</span><span class="">, </span><span class="dv">7</span><span class="">)]), (</span><span class="st">&quot;A&quot;</span><span class="">, [(</span><span class="st">&quot;E&quot;</span><span class="">, </span><span class="dv">4</span><span class="">), (</span><span class="st">&quot;A&quot;</span><span class="">, </span><span class="dv">6</span><span class="">)])]</span>
</code></pre></div><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="">λ</span><span class="op">&gt;</span><span class=""> </span><span class="op">:</span><span class="">m </span><span class="op">+</span><span class=""> </span><span class="dt">System.Random.Mersenne.Pure64</span>
<span class="">λ</span><span class="op">&gt;</span><span class=""> gen </span><span class="ot">&lt;-</span><span class=""> newPureMT</span>
<span class="">λ</span><span class="op">&gt;</span><span class=""> runMarkov </span><span class="dv">15</span><span class=""> example gen </span><span class="st">&quot;E&quot;</span>
<span class="dt">Right</span><span class=""> [</span><span class="st">&quot;E&quot;</span><span class="">,</span><span class="st">&quot;A&quot;</span><span class="">,</span><span class="st">&quot;A&quot;</span><span class="">,</span><span class="st">&quot;A&quot;</span><span class="">,</span><span class="st">&quot;E&quot;</span><span class="">,</span><span class="st">&quot;E&quot;</span><span class="">,</span><span class="st">&quot;A&quot;</span><span class="">,</span><span class="st">&quot;A&quot;</span><span class="">,</span><span class="st">&quot;A&quot;</span><span class="">,</span><span class="st">&quot;A&quot;</span><span class="">,</span><span class="st">&quot;A&quot;</span><span class="">,</span><span class="st">&quot;A&quot;</span><span class="">,</span><span class="st">&quot;E&quot;</span><span class="">,</span><span class="st">&quot;E&quot;</span><span class="">,</span><span class="st">&quot;A&quot;</span><span class="">]</span>
</code></pre></div><p>The Markov chain passes through “A” a bit more often than “E”, which is to be expected from its definition.</p><h3 id="towards-a-sentence-generator">Towards a Sentence Generator</h3><p>The process of sentence generation using Markov chains is pretty simple: For each word in a “seed text,” find the probability of each other word proceeding it. Build a Markov chain out of these probabilities, using words as states, and run it a desired number of times. In order to do this, we’ll first need a utility function which takes pairs of elements (which will represent words along with the word following them in a “seed text”) and produces a Markov chain out of them.</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="ot">insertMkvPairsInto ::</span><span class=""> (</span><span class="dt">Hashable</span><span class=""> a, </span><span class="dt">Eq</span><span class=""> a) </span><span class="ot">=&gt;</span><span class=""> </span><span class="dt">MarkovI</span><span class=""> a </span><span class="ot">-&gt;</span><span class=""> [(a, a)] </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">MarkovI</span><span class=""> a</span>
<span class="">insertMkvPairsInto mkv [] </span><span class="ot">=</span><span class=""> mkv</span>
<span class="">insertMkvPairsInto mkv ps </span><span class="ot">=</span><span class=""> insertEnd lst </span><span class="op">$</span><span class=""> foldl&#39; (</span><span class="fu">flip</span><span class=""> (</span><span class="fu">uncurry</span><span class=""> (insertMkvI </span><span class="dv">1</span><span class="">))) mkv ps</span>
<span class="">  </span><span class="kw">where</span><span class=""> lst </span><span class="ot">=</span><span class=""> </span><span class="fu">snd</span><span class=""> </span><span class="op">$</span><span class=""> </span><span class="fu">last</span><span class=""> ps</span>
</code></pre></div><p>For each pair <code>(x, y)</code>, we insert a transition <code>x -&gt; y</code> with weight 1 into the Markov chain, and insert the final value in as an <code>End</code>. The reason this works is because of something I mentioned earlier: <code>Rand</code> handles distributions like <code>[(True, 1), (True, 1), (False, 1)]</code> properly. We build lists very similar to this one when processing a block of text for sentence generation, and when finally converting to <code>Markov</code>, all of that plumbing gets handled automatically. As a final note, we’ll use that <code>End</code> construct to mark the end of a sentence.</p><p>The next thing is actually building a <code>MarkovI</code> from a sentence – this can be done by zipping the list of its words with the tail of it and using the aforementioned function, like so:</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="kw">import</span><span class=""> </span><span class="kw">qualified</span><span class=""> </span><span class="dt">Data.Text</span><span class=""> </span><span class="kw">as</span><span class=""> </span><span class="dt">T</span>
<span class="ot">wordPairs ::</span><span class=""> </span><span class="dt">T.Text</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> [(</span><span class="dt">T.Text</span><span class="">, </span><span class="dt">T.Text</span><span class="">)]</span>
<span class="">wordPairs </span><span class="ot">=</span><span class=""> (</span><span class="fu">zip</span><span class=""> </span><span class="op">&lt;*&gt;</span><span class=""> </span><span class="fu">tail</span><span class="">) </span><span class="op">.</span><span class=""> T.words</span>

<span class="ot">insertSentence ::</span><span class=""> </span><span class="dt">MarkovI</span><span class=""> </span><span class="dt">T.Text</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">T.Text</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">MarkovI</span><span class=""> </span><span class="dt">T.Text</span>
<span class="">insertSentence mkv </span><span class="ot">=</span><span class=""> insertMkvPairsInto mkv </span><span class="op">.</span><span class=""> wordPairs</span>
</code></pre></div><small>wordPairs could be written more simply in a pointful style, but I think the point{free, less} version is cool. :)</small>
<p>Now, to build a Markov chain from a bunch of sentences (be it a paragraph, a book), we can just fold into an empty <code>MarkovI</code> and convert it from the intermediate representation:</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="ot">fromSentences ::</span><span class=""> </span><span class="dt">R.RandomGen</span><span class=""> g </span><span class="ot">=&gt;</span><span class=""> [</span><span class="dt">T.Text</span><span class="">] </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">Markov</span><span class=""> g </span><span class="dt">T.Text</span>
<span class="">fromSentences </span><span class="ot">=</span><span class=""> fromMarkovI </span><span class="op">.</span><span class=""> foldl&#39; insertSentence M.empty</span>
</code></pre></div><p>The rest is mostly plumbing:</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="ot">{-# LANGUAGE OverloadedStrings #-}</span>
<span class="kw">import</span><span class=""> </span><span class="dt">System.Random.Mersenne.Pure64</span>

<span class="ot">runFromSentences ::</span><span class=""> </span><span class="dt">Int</span><span class=""> </span><span class="ot">-&gt;</span><span class=""> [</span><span class="dt">T.Text</span><span class="">] </span><span class="ot">-&gt;</span><span class=""> </span><span class="dt">IO</span><span class=""> (</span><span class="dt">Either</span><span class=""> </span><span class="dt">Err</span><span class=""> </span><span class="dt">T.Text</span><span class="">)</span>
<span class="">runFromSentences n sentences </span><span class="ot">=</span><span class=""> </span><span class="kw">do</span>
<span class="">  g </span><span class="ot">&lt;-</span><span class=""> newPureMT</span>
<span class="">  </span><span class="kw">let</span><span class=""> hds </span><span class="ot">=</span><span class=""> </span><span class="fu">map</span><span class=""> (</span><span class="fu">head</span><span class=""> </span><span class="op">.</span><span class=""> T.words) sentences</span>
<span class="">  seed </span><span class="ot">&lt;-</span><span class=""> R.uniform hds</span>
<span class="">  </span><span class="fu">return</span><span class=""> </span><span class="op">$</span><span class=""> T.unwords </span><span class="op">&lt;$&gt;</span><span class=""> runMarkov n (fromSentences sentences) g seed</span>

<span class="ot">test ::</span><span class=""> [</span><span class="dt">T.Text</span><span class="">]</span>
<span class="">test </span><span class="ot">=</span><span class=""> [</span>
<span class="">  </span><span class="st">&quot;I am a monster.&quot;</span><span class="">,</span>
<span class="">  </span><span class="st">&quot;I am a rock star.&quot;</span><span class="">,</span>
<span class="">  </span><span class="st">&quot;I want to go to Hawaii.&quot;</span><span class="">,</span>
<span class="">  </span><span class="st">&quot;I want to eat a hamburger.&quot;</span><span class="">,</span>
<span class="">  </span><span class="st">&quot;I have a really big headache.&quot;</span><span class="">,</span>
<span class="">  </span><span class="st">&quot;Haskell is a fun language!&quot;</span><span class="">,</span>
<span class="">  </span><span class="st">&quot;Go eat a big hamburger!&quot;</span><span class="">,</span>
<span class="">  </span><span class="st">&quot;Markov chains are fun to use!&quot;</span>
<span class="">  ]</span>
</code></pre></div><p>We get a new <a href="http://hackage.haskell.org/package/mersenne-random-pure64-0.2.0.2/docs/System-Random-Mersenne-Pure64.html"><code>PureMT</code></a> to use for a generator, and grab a random word (from the beginning of a sentence) to use as the starting state. We then run a markov simulation, collecting the words we pass through, and finally call <code>T.unwords</code> on the result to build a sentence from the words in sequence. Running this yields some interesting statements (and a lot of nonsensical ones), for example:</p><div class="pandoc-code highlighted"><pre><code class="haskell"><span class="">λ</span><span class="op">&gt;</span><span class=""> runFromSentences </span><span class="dv">10</span><span class=""> test</span>
<span class="dt">Right</span><span class=""> </span><span class="st">&quot;Haskell is a hamburger.&quot;</span>
<span class="dt">Right</span><span class=""> </span><span class="st">&quot;Go eat a really big headache.&quot;</span>
<span class="dt">Right</span><span class=""> </span><span class="st">&quot;I am a fun to go to go to eat&quot;</span>
</code></pre></div><h3 id="application-rap-candy">Application: Rap Candy</h3><p>As you might imagine, this type of thing gets more interesting when you’re working with a larger set of sentences. For me, I thought it would be fun(ny) to take lines from Eminem’s music as “sentences,” make tweet-sized snippets from them, and automate a twitter bot to post one every day. Most of the tweets are pretty nonsensical (and very vulgar), here’s one:</p><blockquote class="twitter-tweet" lang="en"><p>Now I&#39;m on a magazine&#10;Take a catastrophe for me&#10;Cause of New sh*t, exclusive whoo kid&#10;I&#39;m the first king of danger, intertwine it</p>&mdash; rapcandy (@_rapcandy) <a href="https://twitter.com/_rapcandy/statuses/495019304883331072">August 1, 2014</a></blockquote>
<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><a href="https://github.com/5outh/rapcandy">rapcandy is open source</a>. Its Markov chain mechanism differs slightly from what was presented here, but the ideas are the same. It also includes a simple example of how to connect to Twitter using Haskell (which I’ll be covering separately in a short blog post soon), as well as web-scraper written in node that I used to download Eminem’s lyrics programmatically. Feel free to browse the code and follow <a href="https://twitter.com/_rapcandy">@_rapcandy on Twitter</a>.</p><p>I’ve also boxed up (most of) the code from this blog post into a small cabal package that you can use if you’d like to play around with your own Markov chain based applications. <a href="https://github.com/5outh/markov-sim">You can download markov-sim and browse its source here</a>.</p></div><div class="metadata"><div class="date" title="Zettel date"><time datetime="2014-08-04">2014-08-04</time></div></div></article><nav class="ui bottom attached segment deemphasized backlinks-container"><h3 class="ui header">Backlinks</h3><ul class="backlinks"><li><span class="zettel-link-container cf"><span class="zettel-link" data-inverted="" data-position="right center" data-tooltip="Tags: home"><a href=".">Ben Kovach</a></span></span><ul class="context-list" style="zoom: 85%;"><li class="item"><div class="pandoc">Linking by tag: <code>Essays</code></div></li></ul></li></ul><a class="ui right ribbon label zettel-tag " href="search.html?tag=Haskell" title="See all zettels tagged &#39;Haskell&#39;">Haskell</a><p></p><a class="ui right ribbon label zettel-tag " href="search.html?tag=Generative" title="See all zettels tagged &#39;Generative&#39;">Generative</a><p></p><a class="ui right ribbon label zettel-tag " href="search.html?tag=Essays" title="See all zettels tagged &#39;Essays&#39;">Essays</a><p></p></nav></div></div><div class="ui one column grid footer-version"><div class="center aligned column"><div class="ui tiny image"><a href="https://neuron.zettel.page"><img alt="logo" src="https://raw.githubusercontent.com/srid/neuron/master/assets/neuron.svg" title="Generated by Neuron" /></a></div></div></div></div></body></html>